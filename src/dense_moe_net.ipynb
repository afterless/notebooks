{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df873e8-d5b3-404d-afaf-eb606807ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afterless/projects/notebooks/.venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import math\n",
    "import random\n",
    "import wandb\n",
    "import json\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import optim, nn, arange\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e16db3-94e1-4f78-a613-a5d9fc1307b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618038c-5059-4567-beb5-0adc54f9c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import string\n",
    "import json\n",
    "\n",
    "def get_stats(ids, c = None):\n",
    "    counts = {} if c is None else c\n",
    "    # iter consecutive elements\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Simple BPE implementation following Karpathy / GPT-4 to avoid cross word splits\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "        self.merges = None\n",
    "        self.BASE = string.printable\n",
    "\n",
    "    def train(self, file_name: str, vocab_size:int) -> None:\n",
    "\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        print(\"1: Have read file \", file_name)\n",
    "\n",
    "        # I've been using a peel from project gutenberg that\n",
    "        # replaces accented characters with generic, drops other\n",
    "        # characters not in string.printable, and excludes\n",
    "        # texts with more than 1 in 100,000 thin\n",
    "        for c in text:\n",
    "            if c not in self.BASE:\n",
    "                raise Exception(\"Not in BASE!\")\n",
    "        print(\"2: Verified all in BASE\")\n",
    "\n",
    "        assert vocab_size >= len(self.BASE)\n",
    "        num_merges = vocab_size - len(self.BASE)\n",
    "\n",
    "        text_chunks = re.findall(GPT4_SPLIT_PATTERN, text)\n",
    "        ids = [[ self.BASE.index(c) for c in text ] for text in text_chunks ]\n",
    "\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: self.BASE[idx] for idx in range(len(self.BASE))}\n",
    "    \n",
    "\n",
    "        for i in range(num_merges):\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                get_stats(chunk_ids, stats)\n",
    "                \n",
    "            pair = max(stats, key=stats.get)\n",
    "            if not pair:\n",
    "                break\n",
    "            idx = len(self.BASE) + i\n",
    "\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def save(self, file_name):\n",
    "        if self.merges is None or self.vocab is None:\n",
    "            raise Exception(\"\")\n",
    "        with open(file_name, 'w') as f:\n",
    "            merges = { (str(k[0]) + \"-\" + str(k[1])) : v for k, v in m.merges.items() }\n",
    "            vocab = { str(k): v for k, v in self.vocab.items() }\n",
    "            to_save = { \"merges\": merges, \"vocab\": vocab }\n",
    "            json.dump(to_save, f, indent=4)\n",
    "\n",
    "    def load(self, file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            self.merges = { tuple(map(int, k.split('-'))): v for k, v in data[\"merges\"].items() }\n",
    "            self.vocab = { int(k): v for k, v in data[\"vocab\"].items() }\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.vocab[idx] for idx in ids)\n",
    "\n",
    "\n",
    "    def _encode_chunk(self, text):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = [ self.BASE.index(c) for c in text ]\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        text_chunks = re.findall(GPT4_SPLIT_PATTERN, text)\n",
    "        ids = []\n",
    "        print(\"Text length: \", len(text), len(text_chunks))\n",
    "        for i, ch in enumerate(text_chunks):\n",
    "            chunk_ids = self._encode_chunk(ch)\n",
    "            if i % 5000000 == 0:\n",
    "                print(i)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d4223-de23-4023-a88c-f8e34fdeb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def get_text_files(folder_path):\n",
    "    text_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            text_files.append(os.path.join(folder_path, file))\n",
    "    return text_files\n",
    "\n",
    "def make_text_dataset(folder_name, train_name, test_name, tokenizer, ratio_mod=300, ctx_len=841):\n",
    "\n",
    "        file_names = get_text_files(folder_name)\n",
    "\n",
    "        print(\"Doing file names \", file_names)\n",
    "        data = []\n",
    "        data_test = []\n",
    "        for i, file_name in enumerate(file_names):\n",
    "            print(\"\\n\\n\", i)\n",
    "            with open(file_name, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            print(\"1. Read file: \", file_name)\n",
    "            print(\" - file length: \", len(text))\n",
    "    \n",
    "            # Filter out non-string characters\n",
    "            print(\"2. Tokenizing: \")\n",
    "            t = time.time()\n",
    "            tokens = tokenizer.encode(text)\n",
    "            print(\" - tokenized length: \", len(tokens))\n",
    "            print(\" - time \", time.time() - t)\n",
    "    \n",
    "            # Create chunks of ctx_len\n",
    "            i = 0\n",
    "            while((i + 1) * ctx_len < len(tokens)):\n",
    "                if i % ratio_mod == 0:\n",
    "                    data_test.append(tokens[i * ctx_len:(i + 1) * ctx_len])\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    data.append(tokens[i * ctx_len:(i + 1) * ctx_len])\n",
    "                    i = i + 1\n",
    "            print(\"3. Chunked into train and test: \", len(data), len(data[0]))\n",
    "\n",
    "        random.shuffle(data)\n",
    "        random.shuffle(data_test)\n",
    "\n",
    "\n",
    "        train_stacked = torch.tensor(data, dtype=torch.short)\n",
    "        test_stacked = torch.tensor(data_test, dtype=torch.short)\n",
    "        print(\"train_stacked\", train_stacked.size())\n",
    "        print(\"test_stacked\", test_stacked.size())\n",
    "\n",
    "        torch.save(train_stacked, train_name)\n",
    "        torch.save(test_stacked, test_name)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a98980-2900-4a78-b89a-7b1e8a675a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3c89c-26ab-4fcf-98b0-931164b38a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor(tensor):\n",
    "    assert len(tensor.size()) == 1\n",
    "    strg = ''\n",
    "    for i in range(len(tensor)):\n",
    "        strg = strg + string.printable[tensor[i].item()]\n",
    "    return strg\n",
    "\n",
    "def show_texts(file_name, tokenizer, num_to_show=30):\n",
    "    tensor = torch.load(file_name)\n",
    "\n",
    "    for i in range(num_to_show):\n",
    "        row = tensor[i]\n",
    "        strg = tokenizer.decode(row.tolist())\n",
    "        print(\"\\n\\n\", i, \"\\n\\n\", row.size())\n",
    "        print(strg)\n",
    "\n",
    "show_texts(\"full_test.pt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef59bc1-0356-41a5-9b99-be94058d9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla causal attention mask + hard-alibi\n",
    "# ------------------------------------------\n",
    "# Hard Alibi (https://arxiv.org/pdf/2402.01032.pdf) is a variant of\n",
    "# the Alibi position encoding (https://arxiv.org/pdf/2108.12409.pdf)\n",
    "# where Alibi's slow linear decay over the attention is replaced by\n",
    "# a discrete decay such that\n",
    "# - the first head sees one token back\n",
    "# - the second, two\n",
    "# - the third, four, and so on\n",
    "# ------\n",
    "# Init:\n",
    "# - config[\"max_length\"] = maximum length sequence fed into it\n",
    "# Forward:\n",
    "# - takes [b, t, t] or [b, nh, t, t]\n",
    "#\n",
    "class HardAlibiCausalMask(nn.Module):\n",
    "    def __init__(self, config, short=False):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = msl = config[\"model_max_length\"]\n",
    "        self.n_heads = n_heads = config[\"num_heads\"]\n",
    "        rel = self.get_relative_positions(msl)\n",
    "        stack = []\n",
    "        bases = config[\"head_bases\"]\n",
    "        for i in range(n_heads):\n",
    "            bools = (rel <= 0) & (rel > -int(bases[i]))\n",
    "            ready = torch.where(bools, torch.zeros(rel.shape), float(\"-inf\"))\n",
    "            stack.append(ready)\n",
    "        self.register_buffer(\"values\", torch.stack(stack, dim=0))\n",
    "\n",
    "    def get_relative_positions(self, seq_len):\n",
    "        x = torch.arange(seq_len)[None, :]\n",
    "        y = torch.arange(seq_len)[:, None]\n",
    "        return x - y\n",
    "\n",
    "    def forward(self, att):\n",
    "        sizes = att.size()\n",
    "        t1, t2 = sizes[-2:]\n",
    "        assert t1 == t2, \"attention must be square\"\n",
    "        assert t1 <= self.max_seq_len, \"attention must be smaller than max_seq_length\"\n",
    "        reshaped_att = att.view(-1, self.n_heads, t1, t1)\n",
    "        return reshaped_att + self.values[:,:t1,:t1]\n",
    "\n",
    "def test():\n",
    "    m = torch.rand(2, 4, 5, 5)\n",
    "    m = HardAlibiCausalMask({ \"model_max_length\": 6, \"num_heads\": 4, \"head_bases\": [1,2,4,8] }, short=True)(m)\n",
    "    # To get a feel for how hard-alibi works uncomment\n",
    "    #print(m)\n",
    "    assert m[0][0][0][0] > 0\n",
    "    assert m[0][0][1][1] > 0\n",
    "    assert m[0][0][0][1] == float(\"-inf\")\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e28f8-af73-473d-bd81-4d2916b12cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla multi-headed self-attention implementation\n",
    "# ------------------------------------------------\n",
    "# Init:\n",
    "# - config[\"hidden_dim\"] = int, dimension of input\n",
    "# - config[\"num_heads\"] = int, number of heads, head_size = hidden_dim // num_heads\n",
    "# - config[\"max_length\"] = int, sets masks size for the CausalMask\n",
    "# Forward:\n",
    "# - takes [b, t, c]\n",
    "# - outputs [b, t, c]\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index, short=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.num_heads = nh = config[\"num_heads\"]\n",
    "        self.k = nn.Linear(hd, hd)\n",
    "        self.q = nn.Linear(hd, hd)\n",
    "        self.v = nn.Linear(hd, hd)\n",
    "        self.p = nn.Linear(hd, hd) # Projection from heads to output\n",
    "        self.mask = HardAlibiCausalMask(config, short=short)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "\n",
    "        num_heads = self.num_heads\n",
    "        heads_dim = c // num_heads\n",
    "        k = self.k(x).view(b, t, num_heads, heads_dim).transpose(1, 2)\n",
    "        q = self.q(x).view(b, t, num_heads, heads_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(b, t, num_heads, heads_dim).transpose(1, 2)\n",
    "\n",
    "        # Mult-headed attention: [b, nh, t, hd] @ [b, nh, hd, ] -> [b, nh, t, t]\n",
    "        att = q @ k.transpose(-2, -1)\n",
    "        att = att / math.sqrt(k.size(-1))\n",
    "        att = self.mask(att)\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # [b, nh, t, t] x [b, nh, t, hs] -> [b, nh, t, hs]\n",
    "        y = att @ v\n",
    "        \n",
    "        # [b, nh, t, hs] -> [b, t, nh, hs] -> [b, t, c]\n",
    "        y = y.transpose(1, 2).contiguous().view(b, t, c)\n",
    "        \n",
    "        return self.p(y)\n",
    "\n",
    "def test_simple_att():\n",
    "    m = SelfAttention({\n",
    "        \"hidden_dim\": 32,\n",
    "        \"model_max_length\": 4,\n",
    "        \"num_heads\": 4,\n",
    "    }, 1, short=True).to(\"cuda\")\n",
    "    out = m(torch.randn(16 * 32, 4, 32).to(\"cuda\"))\n",
    "    print(out.size())\n",
    "    print(out.reshape(16 * 32, 4, 32).size())\n",
    "\n",
    "#test_simple_att()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c92b5-9e97-4644-b46f-d1ad99a937d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP module\n",
    "# -------------------------------\n",
    "# Note: Signature of init here matches init for MoE\n",
    "#\n",
    "# Init:\n",
    "# - config[\"hidden_dim\"] = dimension of c in [b, t, c] input\n",
    "# - index = int index\n",
    "# - scaling = amount by which to scale weights\n",
    "# Forward:\n",
    "# - tuple (tensor, aux_loss)\n",
    "# -- tensor is [b, t, c]\n",
    "# -- aux_loss is a scalar added to cross-entropy loss\n",
    "#\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index, scaling=1.0, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.seq = nn.Sequential(*[\n",
    "            nn.Linear(hd, int(hd * expansion_factor)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hd * expansion_factor), hd),\n",
    "        ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.seq[0].weight.data *= scaling\n",
    "            self.seq[2].weight.data *= scaling\n",
    "            self.seq[0].bias.data *= scaling\n",
    "            self.seq[2].bias.data *= scaling   \n",
    "\n",
    "    def forward(self, inp_tuple):\n",
    "        x, aux_loss = inp_tuple\n",
    "        return self.seq(x), aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4aebc9-f146-485c-a2ce-f3de68634a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise multiplies: x * (1 +- eps)\n",
    "# -------------------------------\n",
    "# Init:\n",
    "# - scaling = amount by which 1 varies\n",
    "# Forward:\n",
    "# - any tensor\n",
    "#\n",
    "class UnitCenteredNoise(nn.Module):\n",
    "    def __init__(self, scaling=0.02):\n",
    "        super(UnitCenteredNoise, self).__init__()\n",
    "        self.scaling = scaling\n",
    "        self.base = 1 - (scaling * 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # uniform 1-centered noise\n",
    "            noise = torch.rand(x.size()).to(x.device)\n",
    "            noise_centered = (noise * self.scaling) + self.base\n",
    "            return x * noise_centered\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b83039-e034-481c-b784-f6cfde360c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Switch Mixture-of-Experts Layer\n",
    "# -------------------------------\n",
    "# Note: Signature of init here matches init for MLP\n",
    "#\n",
    "# Init:\n",
    "# - config[\"hidden_dim\"] = Dimension of c in [b, t, c] input\n",
    "# - config[\"num_experts\"] = A layer indexed array of how many\n",
    "#                           experts are per layer\n",
    "# - config[\"init_moe_scaling\"] = Amount by which to scale\n",
    "#                                weights in MoE experts. If you don't\n",
    "#                                make them smaller, then it doesn't \n",
    "#                                learn as well for mysterious reasons.\n",
    "#                                \n",
    "# Forward:\n",
    "# - tuple (tensor, aux_loss)\n",
    "# -- tensor is [b, t, c]\n",
    "# -- aux_loss is a scalar added to cross-entropy loss\n",
    "class SwitchMoE(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_idx = 0\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.num_experts = num_experts = config[\"num_experts\"][index]\n",
    "        self.num_exp_per_token = config[\"num_exp_per_token\"]\n",
    "        self.moe_scaling = moe_scaling = config[\"init_moe_scaling\"]\n",
    "        self.moe_expansion_factor = mef = config[\"moe_expansion_factor\"]\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            MLP(config, index=index, scaling=moe_scaling, expansion_factor=mef)\n",
    "            for index\n",
    "            in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hd, config[\"num_experts\"][index]),\n",
    "            UnitCenteredNoise(scaling=0.02),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm([hd])\n",
    "        \n",
    "\n",
    "    def forward(self, xx):\n",
    "        inp, aux_loss = xx\n",
    "        b, t, c = inp.shape\n",
    "\n",
    "        self.train_idx = self.train_idx + 1\n",
    "\n",
    "        # Reshape to [b * t, c], makes it easier to think about\n",
    "        inp = inp.reshape(b * t, c)\n",
    "        \n",
    "        gate_val_continuous = self.gate(inp) # [b * t, c] -> [b * t, num_gates]\n",
    "        _, gate_val_indices = torch.topk(\n",
    "            gate_val_continuous,\n",
    "            self.num_exp_per_token,\n",
    "            dim=-1\n",
    "        ) # [b * t, num_gates] -> [b * t, 1]\n",
    "        \n",
    "        # Map [b * t, 1] a one-hot [b * t, num_experts] where the last dim is one-hot encoded\n",
    "        one_hot = torch.nn.functional.one_hot(gate_val_indices, num_classes=self.num_experts).sum(1)\n",
    "\n",
    "        # Calculate auxillary loss to balance the experts\n",
    "        f = one_hot.sum(dim=0) # [b * t, num_experts] -> [num_experts]\n",
    "        if self.train_idx % 10000 == 0:\n",
    "            print(f)\n",
    "        f = f / f.sum()\n",
    "        P = gate_val_continuous.sum(dim=0) # [b * t, num_experts] -> [num_experts]\n",
    "        P = P / P.sum()\n",
    "\n",
    "        extra_aux_loss = (P * f).sum() * self.num_experts\n",
    "\n",
    "        output = torch.zeros_like(inp)\n",
    "        for i in range(self.num_experts):\n",
    "            \n",
    "            mask = one_hot[:,i] == 1 # mask shape: [b * t]\n",
    "            mask_expand = mask.unsqueeze(-1).expand_as(output) # to [b * t, c]\n",
    "      \n",
    "            inp_for_expert = inp[mask_expand].reshape(-1, c)\n",
    "            out_from_exp, _ = self.experts[i]((inp_for_expert, torch.zeros([1])))\n",
    "\n",
    "            output[mask_expand] =+ out_from_exp.reshape(-1)\n",
    "          \n",
    "        return self.ln(output.reshape(b, t, c)), extra_aux_loss + aux_loss\n",
    "        \n",
    "        \n",
    "def test():\n",
    "    a = torch.ones(7) / torch.ones(7).sum()\n",
    "    b = torch.ones(7) / torch.ones(7).sum()\n",
    "    print((a * b).sum())\n",
    "    \n",
    "    m = torch.randn(16, 32, 48).to(\"cuda\")\n",
    "    nm = SwitchMoE({\n",
    "        \"hidden_dim\": 48,\n",
    "        \"num_experts\": [8,8,8],\n",
    "        \"init_moe_scaling\": 1.0,\n",
    "        \"moe_expansion_factor\": 4,\n",
    "        \"num_exp_per_token\": 2,\n",
    "        \"extra_fake_experts\": 2,\n",
    "    }, 1).to(\"cuda\")\n",
    "    out = nm((m, torch.zeros([1]).to(\"cuda\")))\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36e9d9-dd79-424a-a139-656d9f9020cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index):\n",
    "        super().__init__()\n",
    "        self.index = index\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.attention = SelfAttention(config, index)\n",
    "\n",
    "        # Switch between a MoE and a MLP layer according to config\n",
    "        self.layer_type = config[\"layer_types\"][index]\n",
    "        if config[\"layer_types\"][index] == \"switch\":\n",
    "            self.ff = SwitchMoE(config, index)\n",
    "        elif config[\"layer_types\"][index] == \"mlp\":\n",
    "            self.ff = MLP(config, index, expansion_factor=config[\"expansion_factors\"][index])\n",
    "        else:\n",
    "            raise Exception(\"Invalid layer type\")\n",
    "\n",
    "        \n",
    "        self.att_norm1 = nn.LayerNorm([hd])\n",
    "        self.att_norm2 = nn.LayerNorm([hd])\n",
    "        self.ff_norm = nn.LayerNorm([hd])\n",
    "\n",
    "    def forward(self, input_tuple):\n",
    "        x, extra_loss = input_tuple\n",
    "\n",
    "        x_att = self.attention(self.att_norm1(x))\n",
    "        x = x + self.att_norm2(x_att)\n",
    "\n",
    "        x_ff, extra_loss = self.ff((self.ff_norm(x), extra_loss))\n",
    "        x = x + x_ff\n",
    "        \n",
    "        return x, extra_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899c3b6-968e-4461-bf0f-35267103cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_dim = self.output_dim = config[\"input_dim\"]\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.model_max_length = config[\"model_max_length\"]\n",
    "        self.layer_types = config[\"layer_types\"]\n",
    "        self.layer_num = layer_num = len(config[\"layer_types\"])\n",
    "        self.num_experts = config[\"num_experts\"] # note this is an array\n",
    "        self.device = config[\"device\"]\n",
    "\n",
    "        self.char_embed = nn.Embedding(self.input_dim, hd)\n",
    "        stack = []\n",
    "        for i in range(layer_num):\n",
    "            stack.append(TransformerBlock(config, i))\n",
    "        self.stack = nn.Sequential(*stack)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hd)\n",
    "        self.to_prob = nn.Linear(hd, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t = len(x), len(x[0])\n",
    "        # Initial input proc: char_embed\n",
    "        inp = self.char_embed(x).to(self.device)\n",
    "        x = inp\n",
    "\n",
    "        # Initialize aux_loss with 0\n",
    "        x, aux_loss = self.stack((x, torch.tensor(0)))\n",
    "\n",
    "        x = self.layer_norm(x.view(b * t, self.hidden_dim))\n",
    "        \n",
    "        return self.to_prob(x).view(b, t, self.output_dim), aux_loss\n",
    "\n",
    "    def generate(self, x, N, temperature=0.1):\n",
    "            x = x.to(self.device)\n",
    "            generated_tokens = []\n",
    "    \n",
    "            for _ in range(N):\n",
    "                logits, _ = self.forward(x)  # Get the output from the forward pass\n",
    "                if temperature == 0:\n",
    "                    # Choose the most likely token deterministically\n",
    "                    next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "                else:\n",
    "                    if temperature != 1.0:\n",
    "                        # Scale the logits by the temperature\n",
    "                        logits = logits / temperature\n",
    "                    probabilities = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)  # Softmax on the last time step\n",
    "                    next_token = torch.multinomial(probabilities, num_samples=1)  # Sample a token from the distribution\n",
    "    \n",
    "                generated_tokens.append(next_token)\n",
    "                \n",
    "                # Update x by appending the next_token\n",
    "                x = torch.cat((x, next_token), dim=1)\n",
    "                if x.shape[1] > self.model_max_length:\n",
    "                    x = x[:, -self.model_max_length:]  # Truncate to max_length if needed\n",
    "    \n",
    "            # Convert the list of tensors to a single tensor\n",
    "            generated_sequence = torch.cat(generated_tokens, dim=1)\n",
    "            return generated_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc01a6-5d9c-499a-a4cc-3e0dced89474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warms up to learning rate over 'steps_up'\n",
    "# Cools down to LR * gamma over 'steps_down'\n",
    "class LinearLRDecayWithWarmup(_LRScheduler):\n",
    "    def __init__(self, optimizer, steps_up, steps_down, gamma, last_epoch=-1):\n",
    "        self.steps_up = steps_up\n",
    "        self.steps_down = steps_down\n",
    "        self.gamma = gamma\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "\n",
    "        if self.last_epoch > self.steps_down + self.steps_up:\n",
    "            return [base_lr * self.gamma for base_lr in self.base_lrs]\n",
    "\n",
    "        if self.last_epoch > self.steps_up:\n",
    "            steps_after_up = self.last_epoch - self.steps_up\n",
    "            percentage_there = steps_after_up / self.steps_down\n",
    "            mult = (1 - percentage_there) * 1 + (percentage_there * self.gamma)\n",
    "            return [base_lr * mult for base_lr in self.base_lrs]\n",
    "            \n",
    "        return [base_lr * (self.last_epoch / self.steps_up) for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ec25c-dd5e-430f-8278-b2cfbeb04adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a text file, tokenizes it extreeemely simply,\n",
    "# and just shoves it into memory as a PyTorch Tensor\n",
    "#\n",
    "# I've been using a ~200 mb scrape of Gutenberg\n",
    "# books to test, but anything of similar size should\n",
    "# work fine\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_name, ctx_len):\n",
    "        self.tensor = torch.load(file_name)\n",
    "        num, t = self.tensor.size()\n",
    "        print(\"Loaded dataset\")\n",
    "        print(\"Datapoints: \", num)\n",
    "        print(\"Length: \", t)\n",
    "        #assert t == ctx_len + 1\n",
    "        self.length = num\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1231c-3cd7-4891-bb7e-bb6075188fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_KEYS = [\n",
    "    # MODEL DETAILS\n",
    "    #\n",
    "    \"layer_types\",\n",
    "    \"clip_grad_norm\",\n",
    "    \"num_experts\",\n",
    "    # Maximum length of input\n",
    "    \"train_max_length\",\n",
    "    \"model_max_length\",\n",
    "    \"head_bases\",\n",
    "    \"hidden_dim\",\n",
    "    # How many input and output classes we have\n",
    "    \"input_dim\",\n",
    "    \"num_heads\",\n",
    "    # Scaling factor for MoE expert weights\n",
    "    \"init_moe_scaling\",\n",
    "    \"moe_expansion_factor\",\n",
    "    \"num_exp_per_token\",\n",
    "    \"expansion_factors\",\n",
    "\n",
    "    # TRAINING DETAILS\n",
    "    #\n",
    "    # Learning rate\n",
    "    \"lr\",\n",
    "    # How long to warm up from 0 lr\n",
    "    \"lr_steps_up\",\n",
    "    # How long to cool down from lr to lr * gamma\n",
    "    \"lr_steps_down\",\n",
    "    # Final learning rate = lr * gamma\n",
    "    \"lr_gamma\", \n",
    "    \"aux_loss_weight\",\n",
    "    \"max_steps\",\n",
    "    # Text file to go through epochs times\n",
    "   \n",
    "    \"file\",\n",
    "    \"file_test\",\n",
    "    \"file_out\",\n",
    "    \"epochs\",\n",
    "    \"batch_size\",\n",
    "    \"log_interval\",\n",
    "    \"device\",\n",
    "    \"test_intervals\",\n",
    "    \"record_prefix\",\n",
    "]\n",
    "\n",
    "def config_to_run_name(config):\n",
    "    return \"__\".join([\n",
    "        \"switch_\" + str(len([x for x in config[\"layer_types\"] if x == \"switch\"])),\n",
    "        \"prm_\" + str(int(config[\"total_params\"] / 1000000)) + \"m\",\n",
    "        \"hd_\" + str(config[\"hidden_dim\"]),\n",
    "        \"layers_\" + str(len(config[\"layer_types\"])),\n",
    "        \"file_\" + str(config[\"file\"]),\n",
    "    ])\n",
    "\n",
    "def verify_config(config):\n",
    "    keys_mandatory = set(CONFIG_KEYS)\n",
    "    keys_used = set(config.keys())\n",
    "    keys_dif = keys_mandatory ^ keys_used\n",
    "    assert len(keys_dif) == 0, f\"Unrecognized or required keys: {keys_dif}\"\n",
    "    assert len(config[\"layer_types\"]) == len(config[\"num_experts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27798823-2b4b-4afe-9a4d-da15d28b0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_mlp_layer(config):\n",
    "\n",
    "    verify_config(config)\n",
    "    model = Transformer(config)\n",
    "    model = model.to(\"cuda\")\n",
    "    \n",
    "    config[\"total_params\"] = tp = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Model Parameters: {(tp / 1000000.0):.2f}m\")\n",
    "    print(\"Config: \", json.dumps(config, indent=4))\n",
    "    \n",
    "    # Load data\n",
    "    dataset = TextDataset(config[\"file\"], ctx_len=config[\"train_max_length\"])\n",
    "    dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Define the loss function, optimizer, param groups\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_test = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    scheduler = LinearLRDecayWithWarmup(\n",
    "        optimizer,\n",
    "        steps_up=config[\"lr_steps_up\"],\n",
    "        steps_down=config[\"lr_steps_down\"],\n",
    "        gamma=config[\"lr_gamma\"]\n",
    "    )\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"moe_vs_dense_v2\",\n",
    "        config=config,\n",
    "        name=config_to_run_name(config)\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    tokens_seen = 0\n",
    "    steps = 0\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            \n",
    "            inputs = batch[:,:-1].to(config[\"device\"]).long()\n",
    "            targets = batch[:,1:].to(config[\"device\"]).long()\n",
    "            outputs, loss_kl = model(inputs)\n",
    "            loss_ce = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss = loss_ce + (loss_kl * config[\"aux_loss_weight\"])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"clip_grad_norm\"])\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            tokens_seen = tokens_seen + len(inputs) * len(inputs[0])\n",
    "            loss_sum += loss_ce.item()\n",
    "            steps += 1\n",
    "            wandb.log({\n",
    "                \"loss\": loss,\n",
    "                \"loss_normal\": loss_ce,\n",
    "                \"loss_kl\": loss_kl,\n",
    "                \"tokens_seen\": tokens_seen,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            if steps in config[\"test_intervals\"]:\n",
    "                print(\"Testing...\")\n",
    "                with torch.no_grad():\n",
    "                    dataset_2 = TextDataset(config[\"file_test\"], ctx_len=config[\"train_max_length\"])\n",
    "                    dataloader_2 = DataLoader(dataset_2, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "                    test_losses = []\n",
    "                    model.eval()\n",
    "                    for i, batch in enumerate(dataloader_2):\n",
    "                        if batch.size()[0] != config[\"batch_size\"]:\n",
    "                            break\n",
    "                        inputs = batch[:,:-1].to(\"cuda\").long()\n",
    "                        targets = batch[:,1:].to(\"cuda\").long()\n",
    "                        outputs, loss_kl = model(inputs)\n",
    "                        loss_extended = criterion_test(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "                        reshaped = loss_extended.reshape(-1, config[\"train_max_length\"])\n",
    "                        test_losses.append(reshaped)\n",
    "                        if i > 4000:\n",
    "                            break\n",
    "                    model.train()\n",
    "                    test_losses_py = torch.concat(test_losses, dim=0)\n",
    "                    print(test_losses_py.size())\n",
    "                    torch.save(test_losses_py, config[\"record_prefix\"] + str(steps).zfill(10) + \".pt\")\n",
    "\n",
    "            # We only care about KL loss instrumentally\n",
    "\n",
    "            if i % config[\"log_interval\"] == 0:\n",
    "                print(i, \", loss: \", loss_sum / config[\"log_interval\"])\n",
    "                loss_sum = 0\n",
    "\n",
    "            if steps > config[\"max_steps\"]:\n",
    "                print(\"Hit max steps\")\n",
    "                break\n",
    "    \n",
    "        print(f\"Epoch {epoch+1} Done\")\n",
    "    torch.save(model.state_dict, config[\"file_out\"])\n",
    "        \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f861fa-2ef6-43f5-af35-77d134a87797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08687d22-483d-4c8a-ac09-fb4b9ac9bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reaches ~??\n",
    "dense_config = {\n",
    "    \"clip_grad_norm\": 0.9,\n",
    "    \"expansion_factors\": [4] * 9,\n",
    "    \"max_steps\": 100000,\n",
    "    \"layer_types\": [\"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\", \"mlp\"],\n",
    "    \"num_experts\": [8] * 9,\n",
    "    \"train_max_length\": 840,\n",
    "    \"model_max_length\": 840,\n",
    "    \"hidden_dim\": 100 * 11,\n",
    "    \"num_exp_per_token\": 1,\n",
    "    \"moe_expansion_factor\": 4,\n",
    "    \"num_heads\": 11,\n",
    "    \"head_bases\": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"input_dim\": 2048,\n",
    "    \"lr_steps_down\": 95000,\n",
    "    \"lr_steps_up\": 2000,\n",
    "    \"lr_gamma\": 0.1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"aux_loss_weight\": 0.08,\n",
    "    \"init_moe_scaling\": 0.0625,\n",
    "    \"record_prefix\": \"dense_\",\n",
    "    \"test_intervals\": [x * 2000 for x in range(50)],\n",
    "    \"file\": \"full_train.pt\",\n",
    "    \"file_test\": \"full_test.pt\",\n",
    "    \"file_out\": \"full_dense_1.pt\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"log_interval\": 2000,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "run_with_mlp_layer(dense_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263065b-f90c-4c74-a6fe-b7c5b85c9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reaches ~??\n",
    "moe_config = {\n",
    "    \"clip_grad_norm\": 0.9,\n",
    "    \"expansion_factors\": [4] * 9,\n",
    "    \"max_steps\": 100000,\n",
    "    \"layer_types\": [\"mlp\", \"switch\", \"mlp\", \"switch\", \"mlp\", \"switch\", \"mlp\", \"switch\", \"mlp\"],\n",
    "    \"num_experts\": [8] * 9,\n",
    "    \"train_max_length\": 840,\n",
    "    \"model_max_length\": 840,\n",
    "    \"hidden_dim\": 72 * 11,\n",
    "    \"num_exp_per_token\": 1,\n",
    "    \"moe_expansion_factor\": 4,\n",
    "    \"num_heads\": 11,\n",
    "    \"head_bases\": [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"input_dim\": 2048,\n",
    "    \"lr_steps_down\": 95000,\n",
    "    \"lr_steps_up\": 2000,\n",
    "    \"lr_gamma\": 0.1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"aux_loss_weight\": 0.08,\n",
    "    \"init_moe_scaling\": 0.0625,\n",
    "    \"record_prefix\": \"test_\",\n",
    "    \"test_intervals\": [x * 2000 for x in range(50)],\n",
    "    \"file\": \"full_train.pt\",\n",
    "    \"file_test\": \"full_test.pt\",\n",
    "    \"file_out\": \"full_moe_1.pt\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"log_interval\": 2000,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "run_with_mlp_layer(moe_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadd1e8-58ec-4e9c-8ecf-cdd1c21c6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def find_files_with_suffix(folder_path, suffix):\n",
    "    files_with_suffix = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(suffix):\n",
    "                files_with_suffix.append(os.path.join(root, file))\n",
    "    return files_with_suffix\n",
    "\n",
    "def means_std_r2(folder1, folder2):\n",
    "    \"\"\"\n",
    "    return the means, std, and the per-token r2 of each\n",
    "    returns something like\n",
    "    {\n",
    "        mean1: arr\n",
    "        mean2: arr\n",
    "        std1: arr,\n",
    "        std2: arr,\n",
    "        r2: arr\n",
    "    }\n",
    "    \"\"\"\n",
    "    means1 = []\n",
    "    means2 = []\n",
    "    std1 = []\n",
    "    std2 = []\n",
    "    r2 = []\n",
    "\n",
    "    f1 = find_files_with_suffix(folder1, \".pt\")\n",
    "    f2 = find_files_with_suffix(folder2, \".pt\")\n",
    "\n",
    "    assert len(f1) == len(f2), \"not equal lengths\"\n",
    "\n",
    "    f1 = sorted(f1)\n",
    "    f2 = sorted(f2)\n",
    "\n",
    "    for p1, p2 in zip(f1, f2):\n",
    "\n",
    "        py1 = torch.load(p1)\n",
    "        py2 = torch.load(p2)\n",
    "        #print(py1.size(), py2.size())\n",
    "\n",
    "        means1.append(py1.mean().item())\n",
    "        means2.append(py2.mean().item())\n",
    "        std1.append(py1.std().item())\n",
    "        std2.append(py2.std().item())\n",
    "        rsq = r2_score(py1.reshape(-1).cpu().numpy(), py2.reshape(-1).cpu().numpy())\n",
    "        r2.append(rsq)\n",
    "        print(\"goin.\", rsq, p1)\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"means1\": means1,\n",
    "        \"means2\": means2,\n",
    "        \"std1\": std1,\n",
    "        \"std2\": std2,\n",
    "        \"r2\": r2,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "dense_and_dense = means_std_r2(\"./dense_large_results\", \"./dense1_results\")\n",
    "moe1_and_dense = means_std_r2(\"./moe_1_results\", \"./dense1_results\")\n",
    "moe2_and_dense = means_std_r2(\"./moe_2\", \"./dense1_results\")\n",
    "moe1_and_moe2 = means_std_r2(\"./moe_2\", \"./moe_1_results\")\n",
    "moe1_and_dense_large = means_std_r2(\"./moe_1_results\", \"./dense_large_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdebbc1-178d-4eb7-b554-59bb47f364fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_line(ln, label):\n",
    "    leng = len(ln)\n",
    "    x = [(x + 1) * 2000 for x in range(leng)]\n",
    "    print(ln[-1])\n",
    "\n",
    "    #print(mean1)\n",
    "    plt.plot(x, ln, label=label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c24907-927c-4d6d-ab0d-2710dbd04a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_line(dense_and_dense[\"r2\"], \"Per token R^2 between dense and dense large\")\n",
    "show_line(moe1_and_dense[\"r2\"], \"Per token R^2 between moe1 and dense\")\n",
    "show_line(moe2_and_dense[\"r2\"], \"Per token R^2 between moe2 and dense\")\n",
    "show_line(moe1_and_moe2[\"r2\"], \"Per token R^2 between moe1 and moe2\")\n",
    "show_line(moe1_and_dense_large[\"r2\"], \"Per token R^2 between moe1 and dense large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298af2ae-c905-46d7-9fe1-bbfd4b391953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_line(ln, label):\n",
    "    leng = len(ln)\n",
    "    x = [(x + 1) * 2000 for x in range(leng)]\n",
    "\n",
    "    #print(mean1)\n",
    "    plt.plot(x, ln, label=label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
